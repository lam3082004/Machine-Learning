{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-jyrJ2AWqsfB"
   },
   "source": [
    "\n",
    "# Problem\n",
    "## Binary classes\n",
    "In this notebook, we consider the logistic regression problem, i.e linear function for classfication.\n",
    "Given a dataset $\\{x_i, y_i\\}_{i=1}^N$, where each $x_i$ is a feature vector and $y_i \\in \\{0, 1\\}$ a binary label. We want to find the best linear model that fit the given data in terms of the *binary cross entropy (BCE)* metric.\n",
    "\n",
    "Unlike in the linear regression problem, our labels are now constrained in the two values of 0 and 1, we can think of this two value as the probabilities that a given datapoint belong to class 1, i.e. if the label is 1 then the probability of that data point to be in class 1 is 100% - a distribution of class conditioned on datapoints. With this interpretation, if we also constrain the output of our model to be also a distribution, then the use of BCE is apparent: it can be seen as the distance between two distributions, similar to the MSE loss (l2 distance) in the Euclidean space.\n",
    "\n",
    "To transform the output of a linear model to a (binary) distribution, we apply a sigmoid function at the end of the model. It is often used to convert continuous values in $(-\\infty, \\infty)$ to probabilities,\n",
    "\\begin{align}\n",
    "P_{\\theta}(Y=1 | X=x) &= \\frac{1}{1+e^{-\\theta^T x}} \\\\\n",
    "\\Rightarrow \\theta^T x &= \\text{ln}\\frac{P_{\\theta}(Y=1 | X=x) }{P_{\\theta}(Y=0 | X=x) }\n",
    "\\end{align}\n",
    "that is, the output of the linear model is now the logarithm of the probability ratio of the two classes.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDAEAXYZwQI6"
   },
   "source": [
    "## MLE formulation\n",
    "Minimizing the BCE loss with the classification problem is actually MLE under the hood. Denote the output of the model as $f_{\\theta}(x) = (1+\\exp(-\\theta^T x))^{-1}$, then\n",
    "\\begin{align}\n",
    "\\text{arg}\\max_{\\theta}P(\\{x_i, y_i\\}_{i=1}^N | \\theta) &= \\text{arg}\\max_{\\theta} \\prod_{i=1}^N f_\\theta (x_i)^{y_i} (1-f_\\theta (x_i))^{1-{y_i}} \\\\\n",
    "&= \\text{arg}\\max_{\\theta} \\sum_{i=1}^N y_i\\text{ln}f_\\theta (x_i) + (1-{y_i})\\text{ln} (1-f_\\theta (x_i)) \\\\\n",
    "&= \\text{arg}\\min_{\\theta} \\sum_{i=1}^N BCE(f_\\theta (x_i), y_i)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BeX7qrBQzqYE"
   },
   "source": [
    "## Gradient derivation\n",
    "We denote (scalar) logit for the class 1 as $z=\\theta ^T x$, then we have the gradient of the sigmoid, denoted as $\\sigma(\\cdot)$, and the logit $z$ as follow:\n",
    "\\begin{align}\n",
    "\\frac{d\\sigma(z)}{dz} &= \\Big( \\frac{1}{1+e^{-z}} \\Big)^{'} = \\frac{e^{-z}}{(1+e^{-z})^2} \\\\\n",
    "&= \\sigma(z) (1-\\sigma(z))\n",
    "\\end{align}\n",
    "Note that we also have $f_{\\theta}(x) = \\sigma (\\theta^T x)$,\n",
    "we are now ready to calculate the gradient of the BCE objective w.r.t $\\theta$\n",
    "\\begin{align}\n",
    "\\nabla_{\\theta} \\sum_{i=1}^N BCE(f_\\theta (x_i), y_i) &= -\\sum_{i=1}^N\\nabla_\\theta \\big( y_i\\text{ln}f_\\theta (x_i) + (1-{y_i})\\text{ln} (1-f_\\theta (x_i) \\big)\\\\\n",
    "&= -\\sum_{i=1}^N y_i \\frac{\\nabla_\\theta f_\\theta (x_i)}{f_\\theta (x_i)} - (1-{y_i})\\frac{\\nabla_{\\theta}f_\\theta (x_i)}{1-f_\\theta (x_i)} \\\\\n",
    "&= -\\sum_{i=1}^N \\frac{y_i - f_\\theta (x_i)}{f_\\theta (x_i) (1-f_\\theta (x_i))} \\nabla_{\\theta} f_\\theta (x_i) \\\\\n",
    "&=-\\sum_{i=1}^N (y_i - f_\\theta (x_i)) \\nabla_{\\theta} z \\\\\n",
    "&=-\\sum_{i=1}^N (y_i - f_\\theta (x_i)) x_i \\\\\n",
    "&=X^T ( f_\\theta(X) - y)\n",
    "\\end{align}\n",
    "From the above gradient, we can also see that there is no closed form of the optimal solution for the logistic regression problem, because such solution would require the $\\theta$ to diverge to infinity (the sigmoid function tend to 0 and 1 at infinity). In the next section, we will use gradient descent to optimize the logistic regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32ddTg53EOhS"
   },
   "source": [
    "## Multiple classes\n",
    "When the data has more than two classes, i.e. $\\{1, \\dots C\\}$, with some number of classes $C$, then the above formulas is not applicable. In this section, we examine the logistic regression for multiple classes.\n",
    "\n",
    "$X\\in \\mathbb R^{N \\times d}$, $\\theta \\in \\mathbb R^{d \\times C}$, $y \\in \\mathbb R^{N \\times C}$, $y_i$ are one hot vectors. The output are fed through a softmax layer.\n",
    "$$f_\\theta(x)_i = \\frac{e^{\\theta_i^T x}}{\\sum_j^Ce^{\\theta_j ^T x}}$$\n",
    "By using similar arguments, we can show that the MLE of the above model class can be obtained by minimizing the cross entropy loss on the dataset. We will not repeat it here. The objective of the optimization\n",
    "$$L(\\theta, X, y) = -\\sum_{i=1}^N \\sum_j^C y_{ij}\\text{ln} f_{\\theta}(x_i)_j$$\n",
    "Let $z_m = \\theta_m ^T x$ and $Z = \\sum_i e^{z_i}$, then\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\text{ln}f_\\theta(x)_j}{\\partial z_m} = \\frac{Z}{e^{z_j}} \\Big[\n",
    "    \\frac{\\mathbf 1(m=j)Ze^{z_j} - e^{z_j + z_m}}{Z^2}\n",
    "     \\Big] = \\mathbf 1(m=j) - f_{\\theta}(x)_m\n",
    "\\end{align}\n",
    "substitute the above into the gradient of cross entropy\n",
    "\\begin{align}\n",
    "\\nabla_{\\theta_m} L(\\theta, X, y) &= -\\sum_{i=1}^N \\sum_j^C y_{ij}\n",
    "[\\mathbf 1(m=j) - f_{\\theta}(x_i)_m]x_i\\\\\n",
    "\\Rightarrow \\nabla_{\\theta} L(\\theta, X, y) &= -\\sum_{i=1}^N x_i y_i^T (I - \\mathbf 1_C f_{\\theta}(x_i)^T) \\\\\n",
    "&= -\\sum_{i=1}^N x_i (y_i^T - f_{\\theta}(x_i)^T)\\\\\n",
    "&= X^T (f_{\\theta}(X) - y)\n",
    "\\end{align}\n",
    "The gradient looks exactly the same as in the binary class cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "452MfA2Q9LLU"
   },
   "source": [
    "# Using logistic regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12426,
     "status": "ok",
     "timestamp": 1728005773522,
     "user": {
      "displayName": "truongthuyuet Trần Trường Thủy",
      "userId": "12851423088823241563"
     },
     "user_tz": -420
    },
    "id": "XfUYvD3vEYsG"
   },
   "outputs": [],
   "source": [
    "#@title import the necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris, load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5A3h2bUN35Wz"
   },
   "source": [
    "## Binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1728005773524,
     "user": {
      "displayName": "truongthuyuet Trần Trường Thủy",
      "userId": "12851423088823241563"
     },
     "user_tz": -420
    },
    "id": "TizDTExlExnx"
   },
   "outputs": [],
   "source": [
    "#@title helper functions\n",
    "def eval(X, y, theta): # Binary Cross Entropy\n",
    "    sigmoid = 1 / (1 + np.exp(-X @ theta) )\n",
    "    bce = -np.mean(\n",
    "        y * np.log(sigmoid + 1e-6) + (1-y) * np.log(1 - sigmoid + 1e-6)\n",
    "    )\n",
    "    return bce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1728005774932,
     "user": {
      "displayName": "truongthuyuet Trần Trường Thủy",
      "userId": "12851423088823241563"
     },
     "user_tz": -420
    },
    "id": "af03FHsLBkwG",
    "outputId": "d9b081b4-887c-43af-b544-3a1584f8dd3c"
   },
   "outputs": [],
   "source": [
    "#@title Load data\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "print('X, y', X.shape, y.shape)\n",
    "\n",
    "# split the train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                       test_size=0.2,\n",
    "                                       random_state=23)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "type(X_train), type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 925,
     "status": "ok",
     "timestamp": 1728005820803,
     "user": {
      "displayName": "truongthuyuet Trần Trường Thủy",
      "userId": "12851423088823241563"
     },
     "user_tz": -420
    },
    "id": "Q9h64z63d9vw",
    "outputId": "4067d2e8-fee4-40cf-c390-33f9f90e2914"
   },
   "outputs": [],
   "source": [
    "# @title Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf = GaussianNB()\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Prediction\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Gauss Naive Bayes model accuracy os Sklearn (in %):\", acc*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "guaGN0Wbp5oB",
    "outputId": "18649bb4-8c15-4a1c-9f91-bae51ac4fa24"
   },
   "outputs": [],
   "source": [
    "# @title LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Prediction\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Logistic Regression model accuracy os Sklearn (in %):\", acc*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 613
    },
    "id": "XQGFHjVrgP1B",
    "outputId": "5f6ae0a2-22cc-4a6c-a85a-1d38f0d7221b"
   },
   "outputs": [],
   "source": [
    "#@title Full Gradient descent\n",
    "assert y_train.max() <= 1\n",
    "\n",
    "lr = 5e-7\n",
    "epoch = 300\n",
    "\n",
    "losses = []\n",
    "\n",
    "# init theta\n",
    "current_theta = np.random.randn((X_train.shape[-1]))/10\n",
    "\n",
    "for _ in range(epoch):\n",
    "    losses.append(eval(X_train, y_train, current_theta))\n",
    "\n",
    "    sigmoid = 1 / (1 + np.exp(-X_train @ current_theta) )\n",
    "    grad = X_train.T @ (sigmoid - y_train)\n",
    "    current_theta -= grad * lr\n",
    "\n",
    "print(\"final loss:\", eval(X_test, y_test, current_theta))\n",
    "plt.plot(losses)\n",
    "plt.title(\"Losses\")\n",
    "plt.show()\n",
    "\n",
    "sigmoid_train = 1 / (1 + np.exp(-X_train @ current_theta) )\n",
    "y_pred = np.round(sigmoid_train)\n",
    "accuracy = np.mean(y_pred == y_train)\n",
    "print(\"train Accuracy:\", accuracy)\n",
    "\n",
    "sigmoid_test = 1 / (1 + np.exp(-X_test @ current_theta) )\n",
    "y_pred = np.round(sigmoid_test)\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(\"test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 506
    },
    "id": "J4b3G7wFInMi",
    "outputId": "83309f7b-05b9-4541-ee4d-40e3d38be4dd"
   },
   "outputs": [],
   "source": [
    "#@title minibatch Gradient descent\n",
    "\n",
    "assert y_train.max() <= 1\n",
    "batch_size = 32\n",
    "lr = 5e-7\n",
    "epoch = 3000\n",
    "\n",
    "losses = []\n",
    "\n",
    "# init theta\n",
    "current_theta = np.random.randn((X_train.shape[-1]))/10\n",
    "\n",
    "for _ in range(epoch):\n",
    "    losses.append(eval(X_train, y_train, current_theta))\n",
    "\n",
    "    indx = np.random.choice(X_train.shape[0], batch_size)\n",
    "    X_batch = X_train[indx]\n",
    "    y_batch = y_train[indx]\n",
    "\n",
    "    sigmoid = 1 / (1 + np.exp(-X_batch @ current_theta) )\n",
    "    grad = -X_batch.T @ (y_batch - sigmoid)\n",
    "    current_theta -= grad * lr\n",
    "\n",
    "print(\"final loss:\", eval(X_test, y_test, current_theta))\n",
    "plt.plot(losses)\n",
    "plt.title(\"Losses\")\n",
    "plt.show()\n",
    "\n",
    "sigmoid_train = 1 / (1 + np.exp(-X_train @ current_theta) )\n",
    "y_pred = np.round(sigmoid_train)\n",
    "accuracy = np.mean(y_pred == y_train)\n",
    "print(\"train Accuracy:\", accuracy)\n",
    "\n",
    "sigmoid_test = 1 / (1 + np.exp(-X_test @ current_theta) )\n",
    "y_pred = np.round(sigmoid_test)\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(\"test Accuracy:\", accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZ9aZs4-ICuj"
   },
   "source": [
    " **Remark:** The training of the gradient descent in our example is very unstable, because the pure gradient descent is noisy. In practice, we usually employ several tricks to make the Gradient descent more robust, for example using momentum, normalizing the data, cliping gradient, using other more sophisticated optimizer such as Adam. In the next section, we will normalize the input data dimension-wise before training with gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "id": "SqQqj-AvIIYn",
    "outputId": "ae34e713-19f2-48d2-b6a1-f1b44c73a136"
   },
   "outputs": [],
   "source": [
    "#@title Full Gradient descent (normalized input)\n",
    "\n",
    "assert y_train.max() <= 1\n",
    "\n",
    "X_mean = np.mean(X_train, axis=0, keepdims=True)\n",
    "X_std = np.std(X_train, axis=0, keepdims=True)\n",
    "\n",
    "X_train = (X_train - X_mean) / (X_std+1e-8)\n",
    "X_test = (X_test - X_mean) / (X_std+1e-8)\n",
    "\n",
    "lr = 1e-3\n",
    "epoch = 300\n",
    "\n",
    "losses = []\n",
    "\n",
    "# init theta\n",
    "current_theta = np.random.randn((X_train.shape[-1]))\n",
    "\n",
    "for _ in range(epoch):\n",
    "    losses.append(eval(X_train, y_train, current_theta))\n",
    "\n",
    "    sigmoid = 1 / (1 + np.exp(-X_train @ current_theta) )\n",
    "    grad = -X_train.T @ (y_train - sigmoid)\n",
    "    current_theta -= grad * lr\n",
    "\n",
    "print(\"final loss:\", eval(X_test, y_test, current_theta))\n",
    "plt.plot(losses)\n",
    "plt.title(\"Losses\")\n",
    "plt.show()\n",
    "\n",
    "sigmoid_train = 1 / (1 + np.exp(-X_train @ current_theta) )\n",
    "y_pred = np.round(sigmoid_train)\n",
    "accuracy = np.mean(y_pred == y_train)\n",
    "print(\"train Accuracy:\", accuracy)\n",
    "\n",
    "sigmoid_test = 1 / (1 + np.exp(-X_test @ current_theta) )\n",
    "y_pred = np.round(sigmoid_test)\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(\"test Accuracy:\", accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "id": "fRUII_HMLKHb",
    "outputId": "3c29bc8c-2551-439a-d5a8-14d3c9e3bfba"
   },
   "outputs": [],
   "source": [
    "#@title minibatch Gradient descent (normalized input)\n",
    "\n",
    "assert y_train.max() <= 1\n",
    "batch_size = 32\n",
    "lr = 1e-3\n",
    "epoch = 3000\n",
    "\n",
    "losses = []\n",
    "\n",
    "# init theta\n",
    "current_theta = np.random.randn((X_train.shape[-1]))\n",
    "\n",
    "for _ in range(epoch):\n",
    "    losses.append(eval(X_train, y_train, current_theta))\n",
    "\n",
    "    indx = np.random.choice(X_train.shape[0], batch_size)\n",
    "    X_batch = X_train[indx]\n",
    "    y_batch = y_train[indx]\n",
    "\n",
    "    sigmoid = 1 / (1 + np.exp(-X_batch @ current_theta) )\n",
    "    grad = -X_batch.T @ (y_batch - sigmoid)\n",
    "    current_theta -= grad * lr\n",
    "\n",
    "print(\"final loss:\", eval(X_test, y_test, current_theta))\n",
    "plt.plot(losses)\n",
    "plt.title(\"Losses\")\n",
    "plt.show()\n",
    "\n",
    "sigmoid_train = 1 / (1 + np.exp(-X_train @ current_theta) )\n",
    "y_pred = np.round(sigmoid_train)\n",
    "accuracy = np.mean(y_pred == y_train)\n",
    "print(\"train Accuracy:\", accuracy)\n",
    "\n",
    "sigmoid_test = 1 / (1 + np.exp(-X_test @ current_theta) )\n",
    "y_pred = np.round(sigmoid_test)\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(\"test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5kUrS3lsLOcD"
   },
   "source": [
    "It is much better by just simply normalizing the input data to an appropriate range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LYeGBgXMs2td"
   },
   "source": [
    "## Multiple classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CJ_LaJ2stLKa"
   },
   "outputs": [],
   "source": [
    "#@title helper functions\n",
    "def eval(X, y, theta):\n",
    "    z = np.exp(X @ theta )\n",
    "    softmax = z / np.sum(z, axis=-1, keepdims=True)\n",
    "    ce = -np.mean(\n",
    "        y * np.log(softmax)\n",
    "    )\n",
    "    return ce\n",
    "\n",
    "def accuracy(X, y, theta):\n",
    "    logits = X @ theta\n",
    "    pred = np.argmax(logits, axis=-1)\n",
    "    pred = pred.reshape(y.shape)\n",
    "    return np.mean(pred == y)\n",
    "\n",
    "def softmax(X, theta):\n",
    "    z = np.exp(X @ theta)\n",
    "    return z / np.sum(z, axis=-1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s9BSIKs0-0ne",
    "outputId": "0d4b3ed0-fe79-442d-de9d-0dc896d69c10"
   },
   "outputs": [],
   "source": [
    "X, y = load_iris(return_X_y=True)\n",
    "# split the train and test dataset\n",
    "n_classes = np.max(y)+1\n",
    "\n",
    "X_train, X_test,\\\n",
    "    y_train, y_test = train_test_split(X, y,\n",
    "                                       test_size=0.20,\n",
    "                                       random_state=23)\n",
    "\n",
    "X_train.shape, y_train.shape, n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GoxRjhwNvDo1",
    "outputId": "26970d08-dab5-484c-bd92-dc4c8fdd22ce"
   },
   "outputs": [],
   "source": [
    "y_train_onehot = np.zeros((y_train.shape[0], n_classes), dtype=int)\n",
    "y_train_onehot[np.arange(y_train.shape[0]), y_train] = 1\n",
    "\n",
    "X_train = np.concatenate([np.ones((X_train.shape[0], 1)), X_train], axis=-1) # add bias\n",
    "X_test = np.concatenate([np.ones((X_test.shape[0], 1)), X_test], axis=-1)\n",
    "\n",
    "y_train_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "E7EiOEka_Nso",
    "outputId": "c8687417-0a1f-4b1c-fb49-b3bfd1b3ab26"
   },
   "outputs": [],
   "source": [
    "#@title Full Gradient descent\n",
    "\n",
    "lr = 1e-4\n",
    "epoch = 3000\n",
    "\n",
    "losses = []\n",
    "\n",
    "# init theta\n",
    "current_theta = np.random.randn(X_train.shape[-1], n_classes)\n",
    "\n",
    "for _ in range(epoch):\n",
    "    losses.append(eval(X_train, y_train_onehot, current_theta))\n",
    "\n",
    "    sm = softmax(X_train, current_theta)\n",
    "    grad = X_train.T @ (sm - y_train_onehot)\n",
    "    current_theta -= grad * lr\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.title(\"Losses\")\n",
    "plt.show()\n",
    "\n",
    "acc = accuracy(X_train, y_train, current_theta)\n",
    "print(\"train Accuracy:\", acc)\n",
    "\n",
    "acc = accuracy(X_test, y_test, current_theta)\n",
    "print(\"test Accuracy:\", acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "XDXF_-4lw_ME",
    "outputId": "a74933f5-2952-466d-b036-a7d80d62706c"
   },
   "outputs": [],
   "source": [
    "#@title minibatch Gradient descent\n",
    "\n",
    "batch_size = 16\n",
    "lr = 1e-4\n",
    "epoch = 3000\n",
    "\n",
    "losses = []\n",
    "\n",
    "# init theta\n",
    "current_theta = np.random.randn(X_train.shape[-1], n_classes)\n",
    "\n",
    "for _ in range(epoch):\n",
    "    losses.append(eval(X_train, y_train_onehot, current_theta))\n",
    "\n",
    "    indx = np.random.choice(X_train.shape[0], batch_size)\n",
    "    X_batch = X_train[indx]\n",
    "    y_batch = y_train_onehot[indx]\n",
    "\n",
    "    sm = softmax(X_batch, current_theta)\n",
    "    grad = X_batch.T @ (sm - y_batch)\n",
    "    current_theta -= grad * lr\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.title(\"Losses\")\n",
    "plt.show()\n",
    "\n",
    "acc = accuracy(X_train, y_train, current_theta)\n",
    "print(\"train Accuracy:\", acc)\n",
    "\n",
    "acc = accuracy(X_test, y_test, current_theta)\n",
    "print(\"test Accuracy:\", acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dDvgFjenCD0z",
    "outputId": "49145340-9e73-45d5-9e82-de28e0a3bb7e"
   },
   "outputs": [],
   "source": [
    "# LogisticRegression\n",
    "clf = LogisticRegression(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "# Prediction\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Logistic Regression model accuracy os Sklearn (in %):\", acc*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RcQkkEV4xpYu"
   },
   "source": [
    "Since the data distribution of the iris dataset is not very skewed, gradient descent can achieve stable performance even without normalizing input data. Though applying the normalization would still increase the convergence rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sj9O8cHMwrE1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
